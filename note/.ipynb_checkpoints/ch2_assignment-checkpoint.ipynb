{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 数据加载器（dataloader）的具体实现\n",
    "- ch2Note中已经写过了简易实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.4.1\n",
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version            #检查是否有torch和tiktoken库\n",
    " \n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader             #导入必须的模块torch,tiktoken和dataset,dataloader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):#定义一个数据集类\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):  #构造函数，要求传入处理的文本，分词器，单个文本块的最大词数，步幅\n",
    "        self.input_ids = []                                  #输入列表\n",
    "        self.target_ids = []                                 #目标列表\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})  #利用分词器，将txt文本先进行分词，保留特殊标记\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):       #按步幅遍历分词后的列表。\n",
    "            input_chunk = token_ids[i:i + max_length]                 #输入为大小为max_length的块\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]       #目标位输入窗口向后滑动一格\n",
    "            self.input_ids.append(torch.tensor(input_chunk))          #向输入列表中加入当前的窗口构成的输入和目标\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)                              #返回当前输入列表中窗口的总数\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]        #返回输入列表，目标列表\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):  #构造数据加载器函数（利用刚刚写的数据集类）\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")  #调用tiktoken库中gpt2的分词器\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) #创建数据集\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)   #调用DataLoader模块，创建一个自己定义的dataloader\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:   #测试，打开一个以有文本\n",
    "    raw_text = f.read()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")         #调用分词器\n",
    "encoded_text = tokenizer.encode(raw_text)         #将完整的文本进行分词\n",
    "\n",
    "vocab_size = 50257                                #定义词汇表的大小\n",
    "output_dim = 256                                  #每一个token对应的向量的维数\n",
    "context_length = 1024                             #文本长度\n",
    "\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)    #创建token的嵌入层\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)  #创建位置的嵌入层\n",
    "\n",
    "max_length = 4           #文本块长度\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length) #调用上方写好的构造函数，创建dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:           #对数据加载器中的每一个批次\n",
    "    x, y = batch                   #x为输入列表，y为目标列表\n",
    "\n",
    "    token_embeddings = token_embedding_layer(x)  #将x通过token的嵌入层，使其中每一个token都变成向量\n",
    "    pos_embeddings = pos_embedding_layer(torch.arange(max_length)) #创建一个位置嵌入，将从[0:max_length-1]的一个序列中，每个数字id转化为一个向量（将具体的位置映射到向量）\n",
    "\n",
    "    input_embeddings = token_embeddings + pos_embeddings #两者相加获得输入层\n",
    "\n",
    "    break  #表示只查看第一批次，不检查全部批次，便于快速调试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(input_embeddings.shape)  #打印刚刚构造好的输入层的大小（最终应该是，文本分层后的[批次，单文本块最大长度，转化向量维度]）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
